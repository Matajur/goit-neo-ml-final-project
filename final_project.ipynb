{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The final project of the 'Machine Learning: Fundamentals and Applications' section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this work is to study the database of anonymized marketing data of a real telecommunications company.\n",
    "\n",
    "The task is performed as a competition on the Kaggle platform.\n",
    "\n",
    "The main goal of the final project of the course is to predict which customers of the company may consider changing their service provider (according to a well-known marketing term, this is \"customer churn\").\n",
    "\n",
    "Thus, the task is to develop an effective predictive model that can process a large number of input features. Demonstrated ability to work with a volume and variety of data that includes both numerical and categorical features while paying attention to class imbalances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Data exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var1</th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>Var9</th>\n",
       "      <th>Var10</th>\n",
       "      <th>...</th>\n",
       "      <th>Var222</th>\n",
       "      <th>Var223</th>\n",
       "      <th>Var224</th>\n",
       "      <th>Var225</th>\n",
       "      <th>Var226</th>\n",
       "      <th>Var227</th>\n",
       "      <th>Var228</th>\n",
       "      <th>Var229</th>\n",
       "      <th>Var230</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>812.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>catzS2D</td>\n",
       "      <td>jySVZNlOJy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xG3x</td>\n",
       "      <td>Aoh3</td>\n",
       "      <td>ZI9m</td>\n",
       "      <td>ib5G6X1eUxUn6</td>\n",
       "      <td>mj86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2688.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>i06ocsg</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kG3k</td>\n",
       "      <td>WqMG</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>55YFVY9</td>\n",
       "      <td>mj86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>P6pu4Vl</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kG3k</td>\n",
       "      <td>Aoh3</td>\n",
       "      <td>ZI9m</td>\n",
       "      <td>R4y5gQQWY8OodqDV</td>\n",
       "      <td>am7c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>BNrD3Yd</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3B1QowC</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WqMG</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Var1  Var2  Var3  Var4  Var5    Var6  Var7  Var8  Var9  Var10  ...  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   812.0  14.0   NaN   NaN    NaN  ...   \n",
       "1   NaN   NaN   NaN   NaN   NaN  2688.0   7.0   NaN   NaN    NaN  ...   \n",
       "2   NaN   NaN   NaN   NaN   NaN  1015.0  14.0   NaN   NaN    NaN  ...   \n",
       "3   NaN   NaN   NaN   NaN   NaN   168.0   0.0   NaN   NaN    NaN  ...   \n",
       "4   NaN   NaN   NaN   NaN   NaN    14.0   0.0   NaN   NaN    NaN  ...   \n",
       "\n",
       "    Var222      Var223  Var224  Var225  Var226  Var227            Var228  \\\n",
       "0  catzS2D  jySVZNlOJy     NaN    xG3x    Aoh3    ZI9m     ib5G6X1eUxUn6   \n",
       "1  i06ocsg  LM8l689qOp     NaN    kG3k    WqMG    RAYp           55YFVY9   \n",
       "2  P6pu4Vl  LM8l689qOp     NaN    kG3k    Aoh3    ZI9m  R4y5gQQWY8OodqDV   \n",
       "3  BNrD3Yd  LM8l689qOp     NaN     NaN    FSa2    RAYp     F2FyR07IdsN7I   \n",
       "4  3B1QowC  LM8l689qOp     NaN     NaN    WqMG    RAYp     F2FyR07IdsN7I   \n",
       "\n",
       "   Var229  Var230  y  \n",
       "0    mj86     NaN  0  \n",
       "1    mj86     NaN  0  \n",
       "2    am7c     NaN  0  \n",
       "3     NaN     NaN  0  \n",
       "4     NaN     NaN  0  \n",
       "\n",
       "[5 rows x 231 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"./datasets/final_proj_data.csv\"\n",
    "raw_data = pd.read_csv(file_path)\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 231)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set consists of 10.000 rows and 231 columns, the last of which is the target feature, which is responsible for whether the customer will switch their provider. Draws attention a large number of gaps in the values in some columns, some of them will be necessary to completely delete, since the number of gaps is too large and the data cannot be restored.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Determination of types of features and existing gaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Columns: 231 entries, Var1 to y\n",
      "dtypes: float64(191), int64(2), object(38)\n",
      "memory usage: 17.6+ MB\n"
     ]
    }
   ],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset includes 193 columns with numerical features, one of which is the target value, and 38 columns with categorical features. Let's see how much is left after cleaning the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Var20     1.0000\n",
       "Var39     1.0000\n",
       "Var32     1.0000\n",
       "Var31     1.0000\n",
       "Var8      1.0000\n",
       "Var15     1.0000\n",
       "Var42     1.0000\n",
       "Var48     1.0000\n",
       "Var52     1.0000\n",
       "Var55     1.0000\n",
       "Var79     1.0000\n",
       "Var230    1.0000\n",
       "Var175    1.0000\n",
       "Var141    1.0000\n",
       "Var167    1.0000\n",
       "Var185    1.0000\n",
       "Var169    1.0000\n",
       "Var209    1.0000\n",
       "Var118    0.9957\n",
       "Var92     0.9957\n",
       "Var190    0.9957\n",
       "Var64     0.9954\n",
       "Var45     0.9922\n",
       "Var102    0.9918\n",
       "Var98     0.9896\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values = raw_data.isna().mean().sort_values(ascending=False)\n",
    "missing_values.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there are many columns in which the number of omissions reaches 100%. Practice shows that data in columns with more than 35% gaps cease to be informative, and it is better to delete them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Var191       1\n",
       "Var215       1\n",
       "Var213       1\n",
       "Var224       1\n",
       "Var211       2\n",
       "Var208       2\n",
       "Var218       2\n",
       "Var201       2\n",
       "Var205       3\n",
       "Var194       3\n",
       "Var225       3\n",
       "Var196       3\n",
       "Var223       4\n",
       "Var229       4\n",
       "Var203       4\n",
       "Var210       6\n",
       "Var227       7\n",
       "Var221       7\n",
       "Var207      12\n",
       "Var219      17\n",
       "Var195      18\n",
       "Var206      21\n",
       "Var226      23\n",
       "Var228      29\n",
       "Var193      40\n",
       "Var212      65\n",
       "Var204     100\n",
       "Var197     185\n",
       "Var192     297\n",
       "Var216     977\n",
       "Var199    1850\n",
       "Var222    2100\n",
       "Var220    2100\n",
       "Var198    2100\n",
       "Var202    3802\n",
       "Var200    4478\n",
       "Var214    4478\n",
       "Var217    5529\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns = raw_data.select_dtypes(include=[\"object\"]).columns\n",
    "unique_values = raw_data[categorical_columns].nunique().sort_values()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, some columns have only one categorical value, making them unsuitable for binary classification. Also, columns in which 50% of the categorical data are unique, firstly, overload the model when encoding categorical features, secondly, they most likely contain personal information, such as names and phone numbers, which in no way affects the client's desire to change provider. Such columns must be deleted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data preparation for numerical columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Removing columns with a large number of NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 72)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_threshold = 0.5\n",
    "columns_to_drop = missing_values[missing_values > nan_threshold].index\n",
    "data_reduced = raw_data.drop(columns=columns_to_drop)\n",
    "data_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Var200    0.4957\n",
       "Var214    0.4957\n",
       "Var94     0.4386\n",
       "Var72     0.4386\n",
       "Var126    0.2780\n",
       "Var24     0.1360\n",
       "Var109    0.1360\n",
       "Var149    0.1360\n",
       "Var119    0.1020\n",
       "Var206    0.1020\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reduced_shape = data_reduced.shape\n",
    "remaining_missing_values = data_reduced.isna().mean().sort_values(ascending=False)\n",
    "remaining_missing_values.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Filling in the NaN for numerical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_columns = data_reduced.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "data_reduced[numerical_columns] = data_reduced[numerical_columns].fillna(\n",
    "    data_reduced[numerical_columns].mean()\n",
    ")\n",
    "\n",
    "missing_values_after_imputation = data_reduced[numerical_columns].isna().sum().sum()\n",
    "missing_values_after_imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaN values in the numerical columns are not present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Check for one-value numerical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_value_numerical_columns = [\n",
    "    col for col in numerical_columns if data_reduced[col].nunique() == 1\n",
    "]\n",
    "\n",
    "single_value_numerical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-value numerical columns are not present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data preparation for categorical columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Removing columns with a large number of unique vlues and with only one value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 61)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_threshold = 100\n",
    "new_categorical_columns = data_reduced.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "single_value_categorical_columns = [\n",
    "    col for col in new_categorical_columns if data_reduced[col].nunique() == 1\n",
    "]\n",
    "high_cardinality_categorical_columns = [\n",
    "    col\n",
    "    for col in new_categorical_columns\n",
    "    if data_reduced[col].nunique() > unique_threshold\n",
    "]\n",
    "\n",
    "columns_to_drop_categorical = (\n",
    "    high_cardinality_categorical_columns + single_value_categorical_columns\n",
    ")\n",
    "data_processed = data_reduced.drop(columns=columns_to_drop_categorical)\n",
    "\n",
    "data_processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Filling in the NaN for categorical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns_remaining = data_processed.select_dtypes(include=[\"object\"]).columns\n",
    "data_processed[categorical_columns_remaining] = data_processed[\n",
    "    categorical_columns_remaining\n",
    "].fillna(data_processed[categorical_columns_remaining].mode().iloc[0])\n",
    "\n",
    "missing_values_categorical_after_imputation = (\n",
    "    data_processed[categorical_columns_remaining].isnull().sum().sum()\n",
    ")\n",
    "missing_values_categorical_after_imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaN values in the categorical columns are not present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Categorical features encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low cardinality categorical features are coded with one hot encoder and others with target encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 99)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, TargetEncoder\n",
    "\n",
    "low_cardinality_cats = [\n",
    "    col for col in categorical_columns_remaining if data_processed[col].nunique() <= 15\n",
    "]\n",
    "moderate_cardinality_cats = [\n",
    "    col for col in categorical_columns_remaining if data_processed[col].nunique() > 15\n",
    "]\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(\n",
    "    drop=\"if_binary\", sparse_output=False, handle_unknown=\"ignore\"\n",
    ").set_output(transform=\"pandas\")\n",
    "one_hot_encoded_data = one_hot_encoder.fit_transform(\n",
    "    data_processed[low_cardinality_cats]\n",
    ")\n",
    "\n",
    "target_encoder = TargetEncoder(random_state=42).set_output(transform=\"pandas\")\n",
    "target_encoded_data = target_encoder.fit_transform(\n",
    "    data_processed[moderate_cardinality_cats], data_processed[\"y\"]\n",
    ")\n",
    "\n",
    "data_encoded = data_processed.drop(\n",
    "    columns=low_cardinality_cats + moderate_cardinality_cats\n",
    ")\n",
    "data_encoded = pd.concat(\n",
    "    [data_encoded, one_hot_encoded_data, target_encoded_data], axis=1\n",
    ")\n",
    "\n",
    "data_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4. Drop outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An object is recognized as an outlier if 20% or more of the object's features are outside Â±3 standard deviations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 99)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "\n",
    "outliers = data_encoded.apply(lambda x: np.abs(zscore(x)).ge(3)).mean(1)\n",
    "\n",
    "out_ind = np.where(outliers > 0.2)[0]\n",
    "\n",
    "data_encoded.drop(out_ind, inplace=True)\n",
    "data_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Class balancing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "0    0.8695\n",
       "1    0.1305\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_distribution = data_encoded[\"y\"].value_counts(normalize=True)\n",
    "class_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of negative results (the customer will not change the provider) significantly exceeds the number of positive ones. Considering the fact that several algorithms are planned to be tested that are sensitive to the class imbalance, it is necessary to balance them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "0    0.5\n",
       "1    0.5\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_encoded.drop(columns=[\"y\"])\n",
    "y = data_encoded[\"y\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Features normalization and model construction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Features standardization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var13</th>\n",
       "      <th>Var21</th>\n",
       "      <th>Var22</th>\n",
       "      <th>Var24</th>\n",
       "      <th>Var25</th>\n",
       "      <th>Var28</th>\n",
       "      <th>Var35</th>\n",
       "      <th>Var38</th>\n",
       "      <th>...</th>\n",
       "      <th>Var227_nIGjgSB</th>\n",
       "      <th>Var227_vJ_w8kB</th>\n",
       "      <th>Var193</th>\n",
       "      <th>Var195</th>\n",
       "      <th>Var204</th>\n",
       "      <th>Var206</th>\n",
       "      <th>Var212</th>\n",
       "      <th>Var219</th>\n",
       "      <th>Var226</th>\n",
       "      <th>Var228</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13912.00</td>\n",
       "      <td>13912.00</td>\n",
       "      <td>13912.00</td>\n",
       "      <td>13912.00</td>\n",
       "      <td>13912.00</td>\n",
       "      <td>13912.00</td>\n",
       "      <td>13912.00</td>\n",
       "      <td>13912.00</td>\n",
       "      <td>13912.00</td>\n",
       "      <td>13912.00</td>\n",
       "      <td>...</td>\n",
       "      <td>13912.00</td>\n",
       "      <td>13912.00</td>\n",
       "      <td>13912.00</td>\n",
       "      <td>13912.00</td>\n",
       "      <td>13912.00</td>\n",
       "      <td>13912.00</td>\n",
       "      <td>13912.00</td>\n",
       "      <td>13912.00</td>\n",
       "      <td>13912.00</td>\n",
       "      <td>13912.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.99</td>\n",
       "      <td>...</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.59</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-2.32</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>-5.74</td>\n",
       "      <td>-2.43</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>-1.30</td>\n",
       "      <td>-5.84</td>\n",
       "      <td>-2.06</td>\n",
       "      <td>-1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.36</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.65</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>33.18</td>\n",
       "      <td>4.71</td>\n",
       "      <td>35.98</td>\n",
       "      <td>39.04</td>\n",
       "      <td>39.01</td>\n",
       "      <td>24.17</td>\n",
       "      <td>34.73</td>\n",
       "      <td>22.73</td>\n",
       "      <td>20.22</td>\n",
       "      <td>5.39</td>\n",
       "      <td>...</td>\n",
       "      <td>40.81</td>\n",
       "      <td>8.16</td>\n",
       "      <td>11.17</td>\n",
       "      <td>38.20</td>\n",
       "      <td>3.83</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2.53</td>\n",
       "      <td>10.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Var6      Var7     Var13     Var21     Var22     Var24     Var25  \\\n",
       "count  13912.00  13912.00  13912.00  13912.00  13912.00  13912.00  13912.00   \n",
       "mean      -0.05     -0.18     -0.14     -0.02     -0.02     -0.03     -0.03   \n",
       "std        0.85      0.89      0.78      0.89      0.89      0.89      0.91   \n",
       "min       -0.59     -1.15     -0.49     -0.46     -0.45     -0.53     -0.48   \n",
       "25%       -0.36     -1.15     -0.49     -0.23     -0.23     -0.53     -0.40   \n",
       "50%       -0.20      0.00     -0.41     -0.16     -0.16     -0.21     -0.21   \n",
       "75%        0.00      0.02      0.00     -0.00      0.00      0.00      0.02   \n",
       "max       33.18      4.71     35.98     39.04     39.01     24.17     34.73   \n",
       "\n",
       "          Var28     Var35     Var38  ...  Var227_nIGjgSB  Var227_vJ_w8kB  \\\n",
       "count  13912.00  13912.00  13912.00  ...        13912.00        13912.00   \n",
       "mean       0.04      0.01      0.05  ...           -0.01           -0.05   \n",
       "std        0.99      0.96      0.99  ...            0.77            0.76   \n",
       "min       -2.32     -0.25     -0.90  ...           -0.02           -0.12   \n",
       "25%       -0.42     -0.25     -0.88  ...           -0.02           -0.12   \n",
       "50%       -0.00     -0.25      0.00  ...           -0.02           -0.12   \n",
       "75%        0.37     -0.25      0.65  ...           -0.02           -0.12   \n",
       "max       22.73     20.22      5.39  ...           40.81            8.16   \n",
       "\n",
       "         Var193    Var195    Var204    Var206    Var212    Var219    Var226  \\\n",
       "count  13912.00  13912.00  13912.00  13912.00  13912.00  13912.00  13912.00   \n",
       "mean       0.24      0.06      0.09      0.26      0.32      0.07      0.09   \n",
       "std        0.83      0.80      0.95      0.95      0.87      0.82      0.91   \n",
       "min       -1.68     -5.74     -2.43     -1.48     -1.30     -5.84     -2.06   \n",
       "25%        0.61      0.17     -0.53     -0.14     -0.18      0.26     -0.59   \n",
       "50%        0.62      0.17      0.08      0.27      0.85      0.27      0.19   \n",
       "75%        0.63      0.18      0.57      0.58      0.87      0.28      0.78   \n",
       "max       11.17     38.20      3.83      2.08      1.75      0.31      2.53   \n",
       "\n",
       "         Var228  \n",
       "count  13912.00  \n",
       "mean       0.27  \n",
       "std        0.85  \n",
       "min       -1.53  \n",
       "25%        0.23  \n",
       "50%        0.72  \n",
       "75%        0.73  \n",
       "max       10.15  \n",
       "\n",
       "[8 rows x 98 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "X_train = pd.DataFrame(\n",
    "    scaler.transform(X_train), columns=X_train.columns, index=X_train.index\n",
    ")\n",
    "X_test = pd.DataFrame(\n",
    "    scaler.transform(X_test), columns=X_test.columns, index=X_test.index\n",
    ")\n",
    "\n",
    "X_train.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.7118714459139991), np.float64(0.8989325409013312))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "rf_balanced_accuracy = balanced_accuracy_score(y_test, rf_predictions)\n",
    "rf_f1 = f1_score(y_test, rf_predictions, average=\"weighted\")\n",
    "\n",
    "rf_balanced_accuracy, rf_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3. Gradient Boosting Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.8129193022810044), np.float64(0.9199318696534544))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_predictions = gb_model.predict(X_test)\n",
    "\n",
    "gb_balanced_accuracy = balanced_accuracy_score(y_test, gb_predictions)\n",
    "gb_f1 = f1_score(y_test, gb_predictions, average=\"weighted\")\n",
    "\n",
    "gb_balanced_accuracy, gb_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Classifier showed much better results on the current dataset. Lut's attempt to improve the score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Data dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1. Extracting feature importances from the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Var126</td>\n",
       "      <td>0.290737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Var212</td>\n",
       "      <td>0.289003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Var218_cJvF</td>\n",
       "      <td>0.057611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Var144</td>\n",
       "      <td>0.057031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Var73</td>\n",
       "      <td>0.045182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Var205_09_Q</td>\n",
       "      <td>0.035295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Var205_sJzTlal</td>\n",
       "      <td>0.032788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Var205_VpdQ</td>\n",
       "      <td>0.030088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Var7</td>\n",
       "      <td>0.028791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Var221_oslk</td>\n",
       "      <td>0.021939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Var228</td>\n",
       "      <td>0.020223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Var206</td>\n",
       "      <td>0.012894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Var140</td>\n",
       "      <td>0.011557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Var74</td>\n",
       "      <td>0.010031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Feature  Importance\n",
       "28          Var126    0.290737\n",
       "94          Var212    0.289003\n",
       "71     Var218_cJvF    0.057611\n",
       "34          Var144    0.057031\n",
       "14           Var73    0.045182\n",
       "48     Var205_09_Q    0.035295\n",
       "50  Var205_sJzTlal    0.032788\n",
       "49     Var205_VpdQ    0.030088\n",
       "1             Var7    0.028791\n",
       "76     Var221_oslk    0.021939\n",
       "97          Var228    0.020223\n",
       "93          Var206    0.012894\n",
       "32          Var140    0.011557\n",
       "15           Var74    0.010031"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame(\n",
    "    {\"Feature\": X_train.columns, \"Importance\": gb_model.feature_importances_}\n",
    ").sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "feature_importances.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2. Elimination of unimportant features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.8142599679650303), np.float64(0.9196327975557692))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_features = feature_importances[\"Feature\"].head(54).values\n",
    "\n",
    "X_train_top = X_train[top_features]\n",
    "X_test_top = X_test[top_features]\n",
    "\n",
    "gb_model_top = GradientBoostingClassifier(random_state=42)\n",
    "gb_model_top.fit(X_train_top, y_train)\n",
    "\n",
    "gb_predictions_top = gb_model_top.predict(X_test_top)\n",
    "gb_balanced_accuracy_top = balanced_accuracy_score(y_test, gb_predictions_top)\n",
    "gb_f1_weighted_top = f1_score(y_test, gb_predictions_top, average=\"weighted\")\n",
    "\n",
    "gb_balanced_accuracy_top, gb_f1_weighted_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elimination of less important features does not increase accuracy, but reduces data dimensionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3. Dimensionality reduction with PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.7680428043597523), np.float64(0.8128348457607095))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "explained_variance_threshold = 0.85\n",
    "\n",
    "pca = PCA(n_components=explained_variance_threshold, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "num_components = X_train_pca.shape[1]\n",
    "\n",
    "gb_model_pca = GradientBoostingClassifier(random_state=42)\n",
    "gb_model_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "gb_predictions_pca = gb_model_pca.predict(X_test_pca)\n",
    "gb_balanced_accuracy_pca = balanced_accuracy_score(y_test, gb_predictions_pca)\n",
    "gb_f1_weighted_pca = f1_score(y_test, gb_predictions_pca, average=\"weighted\")\n",
    "\n",
    "gb_balanced_accuracy_pca, gb_f1_weighted_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction using PCA does not improve model accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Optimization of hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "gb_model_opt = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "grid_search_gb = GridSearchCV(\n",
    "    estimator=gb_model_opt,\n",
    "    param_grid=gb_param_grid,\n",
    "    scoring='balanced_accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "best_params_gb = grid_search_gb.best_params_\n",
    "best_balanced_accuracy_gb = grid_search_gb.best_score_\n",
    "\n",
    "best_params_gb, best_balanced_accuracy_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search predicted that the default settings were best. Let's try to optimize the hyperparameters manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.8501472859506609), np.float64(0.864980426580023))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_model_best = GradientBoostingClassifier(\n",
    "    random_state=42, max_depth=5, learning_rate=0.01, subsample=0.8\n",
    ")\n",
    "\n",
    "gb_model_best.fit(X_train_top, y_train)\n",
    "gb_predictions_final = gb_model_best.predict(X_test_top)\n",
    "\n",
    "\n",
    "gb_balanced_accuracy_final = balanced_accuracy_score(y_test, gb_predictions_final)\n",
    "gb_f1_final = f1_score(y_test, gb_predictions_final, average=\"weighted\")\n",
    "\n",
    "gb_balanced_accuracy_final, gb_f1_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. ML Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class CustomPreprocessorWithEncoding(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nan_threshold=0.5, unique_threshold=100):\n",
    "        self.nan_threshold = nan_threshold\n",
    "        self.unique_threshold = unique_threshold\n",
    "        self.one_hot_encoder = None\n",
    "        self.target_encoder = None\n",
    "        self.columns_to_drop = None\n",
    "        self.numerical_columns = None\n",
    "        self.categorical_columns = None\n",
    "        self.categorical_columns_to_drop = None\n",
    "        self.remaining_categorical_columns = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        # Columns to drop based on nan values threshold\n",
    "        self.columns_to_drop = X.columns[X.isna().mean() > self.nan_threshold].tolist()\n",
    "\n",
    "        # Split numerical and categorical columns\n",
    "        num_columns = X.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "        self.numerical_columns = [\n",
    "            col for col in num_columns if col not in self.columns_to_drop\n",
    "        ]\n",
    "        cat_columns = X.select_dtypes(include=[\"object\"]).columns\n",
    "        self.categorical_columns = [\n",
    "            col for col in cat_columns if col not in self.columns_to_drop\n",
    "        ]\n",
    "\n",
    "        # Columns to drop based on unique values threshold\n",
    "        single_value_cats = [\n",
    "            col for col in self.categorical_columns if X[col].nunique() == 1\n",
    "        ]\n",
    "        high_cardinality_cats = [\n",
    "            col\n",
    "            for col in self.categorical_columns\n",
    "            if X[col].nunique() > self.unique_threshold\n",
    "        ]\n",
    "        self.categorical_columns_to_drop = single_value_cats + high_cardinality_cats\n",
    "        self.remaining_categorical_columns = [\n",
    "            col\n",
    "            for col in self.categorical_columns\n",
    "            if col not in self.categorical_columns_to_drop\n",
    "        ]\n",
    "\n",
    "        # Identify columns for encoding\n",
    "        self.low_cardinality_cats = [\n",
    "            col for col in self.remaining_categorical_columns if X[col].nunique() <= 15\n",
    "        ]\n",
    "        self.moderate_cardinality_cats = [\n",
    "            col\n",
    "            for col in self.remaining_categorical_columns\n",
    "            if 15 < X[col].nunique() <= self.unique_threshold\n",
    "        ]\n",
    "\n",
    "        # Initialize encoders\n",
    "        self.one_hot_encoder = OneHotEncoder(\n",
    "            drop=\"if_binary\", sparse_output=False, handle_unknown=\"ignore\"\n",
    "        ).set_output(transform=\"pandas\")\n",
    "        self.one_hot_encoder.fit(X[self.low_cardinality_cats])\n",
    "\n",
    "        self.target_encoder = TargetEncoder(random_state=42).set_output(\n",
    "            transform=\"pandas\"\n",
    "        )\n",
    "        if y is not None:\n",
    "            self.target_encoder.fit(X[self.moderate_cardinality_cats], y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        # Drop columns with excessive missing and unique values\n",
    "        X = X.drop(\n",
    "            columns=self.columns_to_drop + self.categorical_columns_to_drop,\n",
    "            errors=\"ignore\",\n",
    "        )\n",
    "\n",
    "        # Fill NaN values in numerical features with mean\n",
    "        X[self.numerical_columns] = X[self.numerical_columns].fillna(\n",
    "            X[self.numerical_columns].mean()\n",
    "        )\n",
    "\n",
    "        # Fill NaN values in categorical features with mode\n",
    "        X[self.remaining_categorical_columns] = X[\n",
    "            self.remaining_categorical_columns\n",
    "        ].fillna(X[self.remaining_categorical_columns].mode().iloc[0])\n",
    "\n",
    "        # Apply one-hot encoding and target encoding\n",
    "        one_hot_encoded_data = self.one_hot_encoder.transform(\n",
    "            X[self.low_cardinality_cats]\n",
    "        )\n",
    "\n",
    "        remaining_moderate_cardinality_cats = [\n",
    "            col for col in self.moderate_cardinality_cats if col in X.columns\n",
    "        ]\n",
    "        target_encoded_data = self.target_encoder.transform(\n",
    "            X[remaining_moderate_cardinality_cats]\n",
    "        )\n",
    "\n",
    "        # Drop original categorical columns used in encoding and concatenate encoded columns\n",
    "        X = X.drop(\n",
    "            columns=self.low_cardinality_cats + remaining_moderate_cardinality_cats,\n",
    "            errors=\"ignore\",\n",
    "        )\n",
    "        X = pd.concat([X, one_hot_encoded_data, target_encoded_data], axis=1)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "pipeline = ImbPipeline(\n",
    "    [\n",
    "        (\n",
    "            \"preprocess\",\n",
    "            CustomPreprocessorWithEncoding(nan_threshold=0.5, unique_threshold=100),\n",
    "        ),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            GradientBoostingClassifier(\n",
    "                random_state=42, max_depth=5, learning_rate=0.01, subsample=0.8\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Retraining the model on the full data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\Documents\\GitHub\\Woolf\\Tier_2\\03_machine_learning\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [3] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_file_path = \"./datasets/final_proj_data.csv\"\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "X_train = train_data.drop(columns=[\"y\"])\n",
    "y_train = train_data[\"y\"]\n",
    "\n",
    "test_file_path = \"./datasets/final_proj_test.csv\"\n",
    "X_test = pd.read_csv(test_file_path)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "pipeline_predictions = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. CSV with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(\n",
    "    {\"index\": range(len(pipeline_predictions)), \"y\": pipeline_predictions}\n",
    ")\n",
    "\n",
    "predictions_df.to_csv(\"./datasets/final_proj_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of the described technique, it was possible to obtain an accuracy of 85% on this data set on Kagle. Further experimentation with feature encoding and the use of deep networks may help further improve accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
